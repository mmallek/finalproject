\documentclass[12pt]{article}
\usepackage{geometry}                
\geometry{letterpaper, top=1.5cm, left=2cm}     
\usepackage{url}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{ textcomp }

\renewcommand{\familydefault}{cmss}

\title{Group 2 Final Project}
\author{PUBHLTH 690R: Statistical Modeling and Data Visualization\\
       Individual Project: Maritza Mallek, Regression Trees}

\begin{document}
\maketitle

% Load libraries
<<echo=FALSE, message=FALSE>>=
require(ggplot2)
require(GGally)
require(RCurl)
require(knitr)
theme_set(theme_bw())
@
% New libraries for indpendent project
<<echo=FALSE, message=FALSE>>=
require(rpart)
require(tree)
@
% Load data
<<echo=FALSE>>=
births = getURL("https://raw.githubusercontent.com/mmallek/finalproject/master/ncbirths.txt",ssl.verifypeer=FALSE, followlocation=1L, .opts=curlOptions(sslversion=3))
births = read.table(text=births, sep='\t', header=TRUE)
attach(births)
@

Classification and Regression Trees (CART) are a nonparametric approach used to generate prediction models. Through binary recursive partitioning, this process works by testing each potential independent variable used to build the model and finding the value at which the most meaningful (binary) split between two branches can be created. The recursive aspect occurs as this process is repeated for each branch of the tree until some predefined set of constraints are met, whcih stops the mode.

These trees are popular because they are fairly intuitive to interpret, especially for non-statisticians, including in public health settings. At each node an equality is shown; for a given observation, if the inequality resolves as TRUE, the reader follows the left branch. The alternative to CARTs are typically some form of regression, such as GLM. For a doctor in a hospital, consulting a regression tree is much simpler than generating and interpreting a regression model. Because CARTs are nonparametric, we cannot use the results to explain why certain variable values result in the observed response, but in situations where we are interested in the likelihood and not necessarily interested in how to change an outcome, CARTs are reasonable enough.

Analysts running CART analysis in R typically use either the tree or the rpart package. The tree package uses residuals, similarly to GLMs, while rpart grows the tree using the Gini coefficient of the data. For this analysis, we used the tree package. 

We are interested in weight at birth. For a classification tree, this means the "lowbirthweight" field; for a regression tree, this means the "weight" field. Here we accept the biological significance of a 5.5 lb cutoff for low birth rate, and therefore use the factored and binary variable lowbirthweight. If we look at the full dataset, we can see how many observations of low birth weight there are.


First, we used all the available variables (the full model) to try and fit the data.
<<fullmodel, fig.height=6, fig.width=6, echo=FALSE, eval=FALSE>>=
form1 = lowbirthweight ~ fage + mage + mature + weeks + visits + marital + gained + gender + habit + whitemom
bt1 = tree(form1, data = births)
summary(bt1)
plot(bt1)
text(bt1, cex=0.7)
# Residual mean deviance:  0.3 = 237 / 790
@

The results showed that the predictor `weeks` is by far the most important predictor. However, it is disingenous to use the `weeks` variable to predict low birth weight because if we know the birth weight, we by definition know the duration of pregnancy. 

Given this reasoning, we proceed with the analysis, omitting weeks as a predictor variable. We specify the model as:

<<model2, results='hide'>>=
lowbirthweight ~ fage + mage + mature + visits + marital + gained + gender + habit + whitemom
@

<<echo=FALSE, include=FALSE>>=
form2 = lowbirthweight ~ fage + mage + mature + visits + marital + gained + gender + habit + whitemom
bt2 = tree(form2, data=births)
@

<<>>=
summary(bt2)
plot(bt2)
text(bt2, cex=0.7)
@

Since we didn't specify any rules for R to use to determine when to stop splitting the tree, the above chunk represents the result of the defaults (minimum number of observations per child node = 5, smallest node size = 10, within-node deviance parameter for nodes to be split = 0.01). We could generate a more complex tree by altering the criteria and purposefully overfitting the model.

<<>>=
bt3 = tree(form2, data=births, control=tree.control(nobs=1000, minsize=2, mindev=0.0))
summary(bt3)
plot(bt3)
text(bt3, cex=0.7)
# Residual mean deviance:  0 = 0 / 697
@

Having "grown the tree," the next step is to prune the tree. Cross-validation is the process of using some reserved part of the data to test the quality of the model fit on the main part of the data. The default method of cross-validation in the tree package is 10-fold, in which 10\% of the data is left out of the initial fitting process.

<<>>=
prune1 = cv.tree(bt2)
plot(prune1)
plot(prune.tree(bt2))

# the lowest deviation is found at a tree size of?
(best = which(prune1$dev==min(prune1$dev)))

# next we run another fit
bt2.pm = prune.tree(bt2, best = mean(best), method="misclass")
bt2.pd = prune.tree(bt2, best = mean(best), method="deviance")

summary(bt2.pm)
# Residual mean deviance:  0.525 = 418 / 796
plot(bt2.pm)
text(bt2.pm, cex=0.8)

summary(bt2.pd)
# Residual mean deviance:  0.531 = 423 / 797
plot(bt2.pd)
text(bt2.pd, cex=0.8)
@

How to interpret output tree
Numbers in parentheses are always p(low) p(not low) and value in tree is the best guess for that branch.

<<>>=
# compare tree to second-order GLM
glm1 = glm(form2, data=births, family=binomial)
summary(glm1)
# Residual deviance: 457.24  on 791  degrees of freedom
457.24/791

@

\begin{tabular}{lll}
Residual Mean Deviance & Equation & Output \\
Full Model & 237 / 790 & 0.3 
\end{tabular}

\begin{tabular}{lll}
Misclassification Rate & Equation & Output \\
Full Model & 8 / 800 & 0.0475
\end{tabular}

\subsubsection{References}
http://www.researchmethods.org/CARTIntroTutorial.pdf





\end{document}